{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dados\n",
    "X = np.arange(start=0, stop=9).reshape(3, 3)\n",
    "y = np.array([[1.], [0.], [1.]])\n",
    "\n",
    "# funções de ativação\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def sigmoid(z):\n",
    "    z_ = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z_))\n",
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    return -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        \n",
    "# camadas (cada linha é um neuronio)\n",
    "theta1 = np.ones((3, 3))\n",
    "theta2 = np.ones((1, 3))\n",
    "\n",
    "# forward\n",
    "z1 = np.dot(X, theta1.T)\n",
    "a1 = relu(z1)\n",
    "\n",
    "z2 = np.dot(a1, theta2.T)\n",
    "a2 = sigmoid(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        # array numpy\n",
    "        self.data = np.array(data, dtype=np.float16)\n",
    "        \n",
    "        # gradiente\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float32)\n",
    "        \n",
    "        # função que calcula o gradiente local\n",
    "        self.local_grad = lambda: None \n",
    "        \n",
    "        # guarda os tensores que que participaram da operação\n",
    "        self._prev = set()\n",
    "    \n",
    "    \n",
    "    def dot(self, other):\n",
    "        '''\n",
    "        Faz a operação de produto interno entre dois tensores.\n",
    "        Args:\n",
    "            other (Tensor): Tensor que irá entrar no produto interno.\n",
    "        '''\n",
    "        \n",
    "        # converte \"other\" para tensor caso não seja\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        \n",
    "        # resultado da operação\n",
    "        out = Tensor(np.dot(self.data, other.data))\n",
    "        \n",
    "        def _backward():\n",
    "            # gradiente em relação ao self\n",
    "            self.grad += np.dot(out.grad, other.data.T)\n",
    "            \n",
    "            # gradiente em relação ao other\n",
    "            other.grad += np.dot(self.data.T, out.grad)\n",
    "        \n",
    "        # atualiza os gradientes dos tensores\n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self, other}\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        # armazena a ordem dos tensores\n",
    "        graph = []\n",
    "        visited = set()\n",
    "        \n",
    "        # função que constrói o grafo\n",
    "        def build_graph(tensor):\n",
    "            if tensor not in visited:\n",
    "                visited.add(tensor)\n",
    "                for child in tensor._prev:\n",
    "                    build_graph(child)\n",
    "                graph.append(tensor)\n",
    "        \n",
    "        build_graph(self)\n",
    "        \n",
    "        # inicializa o gradiente do tensor final (loss) como 1\n",
    "        self.grad = np.ones_like(self.data)\n",
    "        \n",
    "        # propaga o gradiente na ordem inversa\n",
    "        for tensor in reversed(graph):\n",
    "            tensor._backward()\n",
    "        \n",
    "        \n",
    "    def __relu__(self):\n",
    "        '''Faz a ativação ReLu.'''\n",
    "        \n",
    "        out = Tensor(np.maximum(0, self.data))\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += (self.data > 0) * out.grad\n",
    "        \n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self}\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "    def __sigmoid__(self):\n",
    "        '''Faz a ativação Sigmoid.'''\n",
    "        \n",
    "        z = np.clip(self.data, -500, 500)\n",
    "        sigmoid_ = 1 / (1 + np.exp(-z))\n",
    "        out = Tensor(sigmoid_)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += sigmoid_ * (1 - sigmoid_) * out.grad \n",
    "            \n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self}\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data - other.data)\n",
    "        else:\n",
    "            return Tensor(self.data - other)\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data / other.data)\n",
    "        else:\n",
    "            return Tensor(self.data / other)\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data)\n",
    "        else:\n",
    "            return Tensor(self.data * other)\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Implementa a operação de soma entre dois Tensores ou um Tensor e um número.\n",
    "        \"\"\"\n",
    "        # Converte \"other\" para Tensor caso não seja\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        \n",
    "        # Resultado da soma\n",
    "        out = Tensor(self.data + other.data)\n",
    "        \n",
    "        def _backward():\n",
    "            # O gradiente da soma é simplesmente repassado para os operandos\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        \n",
    "        # Define o método backward e registra os predecessores\n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self, other}\n",
    "        \n",
    "        return out\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def T(self):\n",
    "        '''Permite fazer a transposição da matriz de dados direto no objeto.'''\n",
    "        return Tensor(self.data.T)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        '''Permite acessar o shape da matriz de dados direto do objeto.'''\n",
    "        return self.data.shape\n",
    "    \n",
    "    def __repr__(self):\n",
    "        ''' Permite imprimir a matriz numpy ao chamar o objeto.'''\n",
    "        return f'{self.data}'\n",
    "\n",
    "\n",
    "\n",
    "def relu(tensor):\n",
    "    return tensor.__relu__()\n",
    "\n",
    "def sigmoid(tensor):\n",
    "    return tensor.__sigmoid__()\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    \"\"\"\n",
    "    Calcula o erro quadrático médio entre a previsão e o alvo.\n",
    "    Args:\n",
    "        pred (Tensor): Valores previstos.\n",
    "        target (Tensor): Valores reais.\n",
    "    Returns:\n",
    "        Tensor: Valor escalar representando o erro.\n",
    "    \"\"\"\n",
    "    # Converte target para Tensor se necessário\n",
    "    target = target if isinstance(target, Tensor) else Tensor(target)\n",
    "    \n",
    "    # (pred - target) ** 2\n",
    "    diff = pred - target\n",
    "    loss = diff * diff / len(target.data)\n",
    "    \n",
    "    def _backward():\n",
    "        # Derivada do MSE em relação a pred: 2 * (pred - target) / n\n",
    "        pred.grad += 2 * diff.data / len(target.data) * loss.grad\n",
    "        # Derivada do MSE em relação a target é oposta a pred\n",
    "        target.grad -= 2 * diff.data / len(target.data) * loss.grad\n",
    "    \n",
    "    loss._backward = _backward\n",
    "    loss._prev = {pred, target}\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data):\n",
    "        # array numpy\n",
    "        self.data = np.array(data, dtype=np.float16)\n",
    "        \n",
    "        # gradiente\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float32)\n",
    "        \n",
    "        # função que calcula o gradiente local\n",
    "        self.local_grad = lambda: None \n",
    "        \n",
    "        # guarda os tensores que que participaram da operação\n",
    "        self._prev = set()\n",
    "    \n",
    "    def dot(self, other):\n",
    "        '''\n",
    "        Faz a operação de produto interno entre dois tensores.\n",
    "        Args:\n",
    "            other (Tensor): Tensor que irá entrar no produto interno.\n",
    "        '''\n",
    "        \n",
    "        # converte \"other\" para tensor caso não seja\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        \n",
    "        # resultado da operação\n",
    "        out = Tensor(np.dot(self.data, other.data))\n",
    "        \n",
    "        def _backward():\n",
    "            # gradiente em relação ao self\n",
    "            self.grad += np.dot(out.grad, other.data.T)\n",
    "            \n",
    "            # gradiente em relação ao other\n",
    "            other.grad += np.dot(self.data.T, out.grad)\n",
    "        \n",
    "        # atualiza os gradientes dos tensores\n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self, other}\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data - other.data)\n",
    "        else:\n",
    "            return Tensor(self.data - other)\n",
    "    def __truediv__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data / other.data)\n",
    "        else:\n",
    "            return Tensor(self.data / other)\n",
    "    def __mul__(self, other):\n",
    "        if isinstance(other, Tensor):\n",
    "            return Tensor(self.data * other.data)\n",
    "        else:\n",
    "            return Tensor(self.data * other)\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Implementa a operação de soma entre dois Tensores ou um Tensor e um número.\n",
    "        \"\"\"\n",
    "        # Converte \"other\" para Tensor caso não seja\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        \n",
    "        # Resultado da soma\n",
    "        out = Tensor(self.data + other.data)\n",
    "        \n",
    "        def _backward():\n",
    "            # O gradiente da soma é simplesmente repassado para os operandos\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        \n",
    "        # Define o método backward e registra os predecessores\n",
    "        out.local_grad = _backward\n",
    "        out._prev = {self, other}\n",
    "        \n",
    "\n",
    "    def __repr__(self):\n",
    "        ''' Permite imprimir a matriz numpy ao chamar o objeto.'''\n",
    "        return f'Tensor({self.data})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, chain=None, parents=None, operation=None):\n",
    "        self.data = np.array(data, dtype=np.float32)\n",
    "        self.grad = np.zeros_like(self.data)\n",
    "        self.operation = operation\n",
    "        self.parents = parents or []\n",
    "        self.chain = chain\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(self.data + other.data, parents=[self, other], operation='add')\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        \n",
    "        out = Tensor(self.data * other.data)\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "            \n",
    "        out.operation = 'mul'\n",
    "        out.parents = [self, other]\n",
    "        out.chain = _backward\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f'Tensor({self.data}, operation={self.operation})'\n",
    "    \n",
    "\n",
    "\n",
    "x = Tensor(np.array([3]))\n",
    "w = Tensor(np.array([2]))\n",
    "b = Tensor(np.array([1]))\n",
    "\n",
    "y = x * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quase certo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(5.0, operation=mse)\n",
      "\n",
      "Tensor([[ 7.]\n",
      " [16.]], operation=add)\n",
      "\n",
      "Tensor([[ 6.]\n",
      " [15.]], operation=matmul)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,1) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[274], line 110\u001b[0m\n\u001b[0;32m    108\u001b[0m pred \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m w\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m    109\u001b[0m loss \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mmse_loss(pred, y)\n\u001b[1;32m--> 110\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[274], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[274], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[274], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     parent\u001b[38;5;241m.\u001b[39mbackward(\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,1) (3,1) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tensor:\n",
    "    def __init__(self, data, parents=None, operation=None):\n",
    "        self.data = np.array(data, dtype=np.float32)  # Assegura que os dados sejam tratados como matrizes\n",
    "        self.grad = np.zeros_like(self.data, dtype=np.float32)  # Inicializa o gradiente com a mesma forma\n",
    "        self.parents = parents or []  # Lista de pais da operação\n",
    "        self.operation = operation  # Tipo de operação (se houver)\n",
    "        \n",
    "    def backward(self, grad=1.0):\n",
    "        self.grad += grad\n",
    "        print(self)\n",
    "        print()\n",
    "        for parent, local_grad in self.parents:\n",
    "            parent.backward(grad *local_grad(self))\n",
    "    \n",
    "    # Função para calcular o erro quadrático médio (MSE)\n",
    "    @staticmethod\n",
    "    def mse_loss(y_pred, y_true):\n",
    "        loss = ((y_pred.data - y_true.data) ** 2).mean()\n",
    "        \n",
    "        return Tensor(\n",
    "            loss, \n",
    "            parents=[\n",
    "                (y_pred, lambda _: 2 * (y_pred.data - y_true.data) / y_pred.data.size), \n",
    "                (y_true, lambda _: -2 * (y_pred.data - y_true.data) / y_true.data.size)],\n",
    "            operation='mse'\n",
    "        )\n",
    "\n",
    "    def __add__(self, other):\n",
    "        '''Permite fazer a operação de adição entre Tensores.'''\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(\n",
    "            self.data + other.data, \n",
    "            parents=[(self, lambda _: 1), (other, lambda _: 1)], \n",
    "            operation='add'\n",
    "        )\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        '''Permite fazer a operação de Multiplicação de Hadamar (elemento com elemento) entre Tensores.'''\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(\n",
    "            self.data * other.data,\n",
    "            parents=[(self, lambda _: other.data), (other, lambda _: self.data)],\n",
    "            operation='mul'\n",
    "        )\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        '''Permite fazer a operação de subtração entre Tensores'''\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(\n",
    "            self.data - other.data,\n",
    "            parents=[(self, lambda _: 1), (other, lambda _: 1)],\n",
    "            operation='sub'\n",
    "        )\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        '''Permite fazer a operação de divisão entre Tensores.'''\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(\n",
    "            self.data / other.data,\n",
    "            parents=[(self, lambda _: 1/other.data), (other, lambda _: -self.data/other.data**2)],\n",
    "            operation='div'\n",
    "        )\n",
    "        \n",
    "    def __matmul__(self, other):\n",
    "        '''Permite fazer o produto interno entre dois Tensores.'''\n",
    "        \n",
    "        other = other if isinstance(other, Tensor) else Tensor(other)\n",
    "        return Tensor(\n",
    "            np.matmul(self.data, other.data),\n",
    "            parents=[(self, lambda _: other.data), (other, lambda _: self.data)],\n",
    "            operation='matmul'\n",
    "        )\n",
    "        \n",
    "    def __repr__(self):\n",
    "        '''Permite visualizar alguns parâmetros do Tensor.'''\n",
    "        \n",
    "        return f'Tensor({str(self.data)}, operation={self.operation})'\n",
    "    \n",
    "    @property\n",
    "    def T(self):\n",
    "        '''Permite fazer a transposição da matriz de dados direto no objeto.'''\n",
    "        \n",
    "        return Tensor(self.data.T)\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        '''Permite acessar o shape da matriz de dados direto do objeto.'''\n",
    "        \n",
    "        return self.data.shape\n",
    "\n",
    "\n",
    "X = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "y = np.array([[10], [15]])\n",
    "\n",
    "w = np.array([[1, 1, 1]])\n",
    "b = np.array([[1], [1]])\n",
    "\n",
    "X = Tensor(X)\n",
    "w = Tensor(w)\n",
    "b = Tensor(b)\n",
    "\n",
    "pred = X @ w.T + b\n",
    "loss = Tensor.mse_loss(pred, y)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(5.0, operation=mse)\n",
      "\n",
      "Tensor([[ 7.]\n",
      " [16.]], operation=add)\n",
      "\n",
      "Tensor([[ 6.]\n",
      " [15.]], operation=matmul)\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,1) (3,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[272], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m pred \u001b[38;5;241m=\u001b[39m X \u001b[38;5;241m@\u001b[39m w\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m+\u001b[39m b\n\u001b[0;32m     12\u001b[0m loss \u001b[38;5;241m=\u001b[39m Tensor\u001b[38;5;241m.\u001b[39mmse_loss(pred, y)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[271], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[271], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     \u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[271], line 15\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, grad)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m parent, local_grad \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparents:\n\u001b[1;32m---> 15\u001b[0m     parent\u001b[38;5;241m.\u001b[39mbackward(\u001b[43mgrad\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocal_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,1) (3,1) "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X @ w.T).grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor([[ 6.]\n",
       " [15.]], operation=matmul)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X @ w.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(node: Tensor):\n",
    "    \"\"\"\n",
    "    Propaga os gradientes para trás através do grafo computacional.\n",
    "    \n",
    "    Args:\n",
    "        node (Tensor): O nó a partir do qual o backward deve começar.\n",
    "    \"\"\"\n",
    "    # Primeiro, exibe o nó atual e o gradiente\n",
    "    print(f\"Backward passando pelo nó: {node}\")\n",
    "    \n",
    "    # Para cada nó, aplicamos a operação de diferenciação e propagação do gradiente\n",
    "    if node.parents:\n",
    "        for parent in node.parents:\n",
    "            # Cada operação tem uma forma diferente de calcular o gradiente.\n",
    "            # Aqui, podemos aplicar a regra da cadeia para cada tipo de operação.\n",
    "\n",
    "            # Exemplo de regra para a operação de multiplicação (mul)\n",
    "            if node.operation == \"mul\":\n",
    "                parent.grad += node.grad * parent.data  # gradiente da multiplicação\n",
    "            # Exemplo de regra para a operação de soma (add)\n",
    "            elif node.operation == \"add\":\n",
    "                parent.grad += node.grad  # gradiente da soma (simples)\n",
    "\n",
    "            # Recursivamente propaga para os pais\n",
    "            backward(parent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tensor([6.], operation=mul), Tensor([1.], operation=None)]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.172], dtype=float16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = 0.01\n",
    "(max(0, y.data + h) - max(0, y.data)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.arange(4.0, requires_grad=True)\n",
    "\n",
    "y = torch.dot(x, x)\n",
    "\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True), tensor([0., 2., 4., 6.]))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(start=0, stop=9).reshape(3, 3)\n",
    "y = np.array([[1.], [0.], [1.]])\n",
    "\n",
    "X = Tensor(X)\n",
    "y = Tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333\n",
      "0.3333\n",
      "0.3333\n"
     ]
    }
   ],
   "source": [
    "tensor1 = Tensor(theta1)\n",
    "tensor2 = Tensor(theta2)\n",
    "\n",
    "for i in range(3):\n",
    "    z1 = X.dot(tensor1.T)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = a1.dot(tensor2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    loss = mse_loss(pred=a2, target=y)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    print(loss.data.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor1.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.84"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m z2 \u001b[38;5;241m=\u001b[39m a1\u001b[38;5;241m.\u001b[39mdot(tensor2\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m      6\u001b[0m a2 \u001b[38;5;241m=\u001b[39m sigmoid(z2)\n\u001b[1;32m----> 8\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43ma2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m()\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sum'"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    z1 = X.dot(tensor1.T)\n",
    "    a1 = relu(z1)\n",
    "\n",
    "    z2 = a1.dot(tensor2.T)\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    loss = ((a2 - y) * (a2 - y)).sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.]\n",
       " [0.]\n",
       " [1.]]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2 * y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.]\n",
       " [1.]\n",
       " [1.]]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ((a2 - y) * (a2 - y)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;241m-\u001b[39m(y \u001b[38;5;241m*\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mlog(a2\u001b[38;5;241m.\u001b[39mdata)) \u001b[38;5;241m+\u001b[39m Tensor(\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m) \u001b[38;5;241m*\u001b[39m Tensor(np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m a2\u001b[38;5;241m.\u001b[39mdata)))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "-(y * Tensor(np.log(a2.data)) + Tensor(1 - y) * Tensor(np.log(1 - a2.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-0.00012339]\n",
       " [ 0.        ]\n",
       " [ 0.        ]]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y * Tensor(np.log(a2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'Tensor' and 'Tensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'Tensor' and 'Tensor'"
     ]
    }
   ],
   "source": [
    "y * Tensor(np.log(a2.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.]\n",
       " [0.]\n",
       " [1.]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
